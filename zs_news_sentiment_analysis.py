# -*- coding: utf-8 -*-
"""ZS_News_Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ca3wwfJtwSS6Y8xeJE0k7ApklIyfoeR
"""

import pandas as pd
import pandas_profiling
import numpy as np
import nltk 
import re

from tqdm import tqdm
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from textblob import TextBlob
from scipy.sparse import hstack, csr_matrix
from sklearn.model_selection import train_test_split                                          
from sklearn.svm import LinearSVR
from sklearn.metrics import mean_absolute_error


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

"""Reading the data."""

train = pd.read_csv('/content/drive/My Drive/zs_news_data/dataset/train_file.csv')
test = pd.read_csv('/content/drive/My Drive/zs_news_data/dataset/test_file.csv')

"""Let's get some insights into the data.
Exploratory Data Analysis(EDA)
"""

train.head()

test.head()

train.describe()

pandas_profiling.ProfileReport(train)

"""From the above profiling we see that there only one column that has missing values i.e. Source"""

train.isnull().sum()  #just crosss checking

test.isnull().sum() #test column also has missing values in the source column only

train['Source'].value_counts()[:5]    #Top 5 values

train['Source'] = train['Source'].fillna('Bloomberg')
test['Source'] = test['Source'].fillna('Bloomberg')

"""Let's check for duplicates as well"""

duplicateRowstrain = train[train.duplicated()] 
duplicateRowstrain

"""Now we will proceed to Text Preprocessing and Cleaning."""

stop_words = set(stopwords.words('english'))

def cleaning_text(text):
  text_token = word_tokenize(text)
  filtered_text = ' '.join([w.lower() for w in text_token if w.lower() not in stop_words and len(w) > 2])
  filtered_text = filtered_text.replace(r"[^a-zA-Z]+", '')
  filtered_text = re.sub(r'\b\d+\b', '', filtered_text)
  clean_text = filtered_text.replace(',', '').replace('.', '').replace(':', '')
  return clean_text

train['Title_Source_Topic'] = train['Title']+ ' ' + train['Source'] + ' ' + train['Topic']
test['Title_Source_Topic'] = test['Title']+ ' ' + test['Source'] + ' ' + test['Topic']

train['Headline_Source_Topic'] = train['Headline'] + ' ' + train['Source'] + ' ' + train['Topic']
test['Headline_Source_Topic'] = test['Headline'] + ' ' + test['Source'] + ' ' + test['Topic']

train['Title_Source_Topic']

train['Title_Source_Topic'][4]

train.tail()

for x in train['Title_Source_Topic']:
  print (x)

train['Title_Source_Topic'] = [cleaning_text(x) for x in tqdm(train['Title_Source_Topic'])]
test['Title_Source_Topic'] = [cleaning_text(x) for x in tqdm(test['Title_Source_Topic'])]

train['Headline_Source_Topic'] = [cleaning_text(x) for x in tqdm(train['Headline_Source_Topic'])]
test['Headline_Source_Topic'] = [cleaning_text(x) for x in tqdm(test['Headline_Source_Topic'])]

vect = TfidfVectorizer(use_idf= True)

train_vect_tts = vect.fit_transform(train['Title_Source_Topic'])
test_vect_tts = vect.transform(test['Title_Source_Topic'])

vect_2 = TfidfVectorizer()

train_vect_2_hst = vect_2.fit_transform(train['Headline_Source_Topic'])
test_vect_2_hst = vect_2.transform(test['Headline_Source_Topic'])

train['polarity_title'] = train['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)
test['polarity_title'] = test['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)

train['subjectivity_title'] = train['Title'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
test['subjectivity_title'] = test['Title'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

train['polarity_headline'] = train['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)
test['polarity_headline'] = test['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)

train['subjectivity_headline'] = train['Headline'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
test['subjectivity_headline'] = test['Headline'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Encoding the categorical variables
encoder = LabelEncoder()

train['Topic'] = encoder.fit_transform(train['Topic'])
test['Topic'] = encoder.transform(test['Topic'])

total = train['Source'].to_list() + test['Source'].to_list()
total = encoder.fit_transform(total)
train['Source'] = encoder.transform(train['Source'])
test['Source'] = encoder.transform(test['Source'])

#Feature scaling

scaler = StandardScaler()

cols = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn']

for col in cols:
  train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))
  test[col] = scaler.transform(test[col].values.reshape(-1, 1))

train.loc[[1185]]

cols_title = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn','polarity_title', 'subjectivity_title']
train_title = train[cols_title]
test_title = test[cols_title]

cols_headline = ['Source', 'Topic', 'Facebook', 'GooglePlus', 'LinkedIn','polarity_headline', 'subjectivity_headline']
train_headline = train[cols_headline]
test_headline = test[cols_headline]

train_title.head()

test_title.head()

train_headline.head()

test_headline.head()

print(np.shape(train_title))
print(np.shape(test_title))

print(np.shape(train_headline))
print(np.shape(test_headline))

print(np.shape(train_vect_tts))
print(np.shape(test_vect_tts))

print(np.shape(train_vect_2_hst))
print(np.shape(test_vect_2_hst))

train_X_Title = hstack([train_vect_tts, csr_matrix(train_title.values)])
test_X_Title = hstack([test_vect_tts, csr_matrix(test_title.values)])
y1 = train['SentimentTitle']

train_X_Headline = hstack([train_vect_2_hst, csr_matrix(train_headline.values)])
test_X_Headline = hstack([test_vect_2_hst, csr_matrix(test_headline.values)])
y2 = train['SentimentHeadline']

np.shape(train_X_Title)

#model for sentiment title
X_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=42)

LSVR1 = LinearSVR(C=0.2)
LSVR1.fit(X_train, y_train)

y_pred1 = LSVR1.predict(X_test)
mae1 = mean_absolute_error(y_pred1, y_test)
print('MAE:', 1 - mae1)

X_train, X_test, y_train, y_test = train_test_split(train_X_Headline, y2, test_size=0.20, random_state=42)

LSVR2 = LinearSVR(C=0.1)
LSVR2.fit(X_train, y_train)

y_pred2 = LSVR2.predict(X_test)
mae2 = mean_absolute_error(y_pred2, y_test)
print('MAE:', 1 - mae2)

print('MAE:', 1 - ((0.4 * mae1) + (0.6 * mae2)))

pred_title = LSVR1.predict(test_X_Title)
pred_headline = LSVR2.predict(test_X_Headline)

test_id = test['IDLink']

df = pd.DataFrame()
df['IDLink'] = test_id
df['SentimentTitle'] = pred_title
df['SentimentHeadline'] = pred_headline
df.to_csv('/content/drive/My Drive/zs_news_data/submission_LSVR.csv', index=False)